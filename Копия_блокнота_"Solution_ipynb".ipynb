{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marianelia/tmp/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22Solution_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFp2aK34okpG"
      },
      "source": [
        "## Лабораторная работа по курсу \"Искусственный интеллект\"\n",
        "## Многослойный персептрон\n",
        "\n",
        "| Студент | Лагуткина |\n",
        "|---------|--------|\n",
        "| Группа  | 6      |\n",
        "\n",
        "Вариант: 1\n",
        "\n",
        "Для начала, скачаем датасет MNIST. Используйте `wget` или `curl`, либо скачайте вручную [по ссылке](https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/MNIST/mnist.pkl.gz)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Нейронная сеть \n",
        "### Установка библиотек и настройка среды\n"
      ],
      "metadata": {
        "id": "04VDWhZ1sJuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "Bcu9rYIaokpQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas\n",
        "import torchvision.utils\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDtD4hoouVP8",
        "outputId": "10b788af-c6cd-434a-a61f-af1ec4d9e507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Устанавливаем вычислительное устройство"
      ],
      "metadata": {
        "id": "b8ZOCpZ-sHhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(torch.cuda.device_count())\n",
        "def print_device():\n",
        "  print('Doing computations on '+device)"
      ],
      "metadata": {
        "id": "sNUj7WZUttXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f4ef7e-942a-448a-cb35-949bce0ac809"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilc-g2EbokpU"
      },
      "source": [
        "## Нейросеть\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bQ0VKw1mokpW"
      },
      "outputs": [],
      "source": [
        "class Perceptron_solution_pytorch(torch.nn.Module):\n",
        "    def __init__(self,learning_rate=0.1,epochs=10, layers=[], trans_f=torch.nn.Tanh(), loss_f=torch.nn.functional.binary_cross_entropy_with_logits):\n",
        "        super().__init__()\n",
        "        self.learning_rate=learning_rate\n",
        "        self.layers=layers\n",
        "        self.transfer=trans_f\n",
        "        self.epochs=epochs\n",
        "        self.loss_function=loss_f\n",
        "        self.level=0\n",
        "\n",
        "    #создание сети и настройка вывода\n",
        "    def create(self, X,Y):\n",
        "        self.level=0\n",
        "        layers_tmp = self.layers.copy()\n",
        "        layers_tmp.insert(0, len(X[0]))\n",
        "        layers_tmp.append(len(np.unique(Y)))\n",
        "        self.classes_count = layers_tmp[-1]\n",
        "        module_layers = []\n",
        "        self.print_format(\"Network Training:\")\n",
        "        self.level += 1\n",
        "        self.print_format(\"In | {}\".format(layers_tmp[0]))\n",
        "        self.print_format(\"Out | {}\".format(layers_tmp[-1]))\n",
        "        self.print_format(\"Hidden layers    | {}\".format(len(layers_tmp)-2))\n",
        "        self.print_format(\"The breadth of hidden layers    | {}\".format(self.layers))\n",
        "        self.level -= 1\n",
        "        for i in range(len(layers_tmp)-2):\n",
        "            #собираем слой\n",
        "            module_layers.append(torch.nn.Linear(layers_tmp[i], layers_tmp[i+1]))\n",
        "            module_layers.append(self.transfer) \n",
        "        module_layers.append(torch.nn.Linear(layers_tmp[-2], layers_tmp[-1]))\n",
        "        self.net = torch.nn.Sequential(*module_layers)\n",
        "        return self      \n",
        "\n",
        "    #создание и обучение сети    \n",
        "    def training(self, X, Y):\n",
        "        self.create(X,Y)\n",
        "        self.level = 0\n",
        "        self.print_format(\"Network Training:\")\n",
        "        self.level += 1\n",
        "\n",
        "        x_value = torch.tensor(np.array(X).astype(np.float32))\n",
        "        y_value = []\n",
        "        for y in Y:\n",
        "            val = np.zeros(self.classes_count)\n",
        "            for i in range(10):\n",
        "                val[i] = 1 if i == y else 0\n",
        "            y_value.append(val.copy().astype(np.float32))\n",
        "        dataset = torch.utils.data.TensorDataset(x_value,torch.tensor(y_value, dtype=torch.float32))\n",
        "        #разделим входные данные на мини-батчи размером 16 \n",
        "        dataloader = torch.utils.data.DataLoader(dataset,batch_size=16)\n",
        "        #оптимизация\n",
        "        optim = torch.optim.Adam(self.net.parameters(),lr=self.learning_rate)\n",
        "        for epoch in range(1, self.epochs+1):\n",
        "            for (x,y) in dataloader:\n",
        "                input = self.net(x)\n",
        "                loss = self.loss_function(input,y) #target=y\n",
        "                optim.zero_grad()\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "            acc = self.accuracy(X,Y)\n",
        "            self.print_format(\"Epoch {} of {}: loss : {}, accuracy : {}\".\n",
        "                              format(epoch, self.epochs, loss, acc))\n",
        "        return self\n",
        "\n",
        "    def print_format(self, msg, level=-1):\n",
        "        if level == -1:\n",
        "            level = self.level\n",
        "        print(\"  \"*level + msg)  \n",
        "\n",
        "    def accuracy_plot(self, acc):\n",
        "        plt.plot(self.level, acc, label='Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    #точность        \n",
        "    def accuracy(self, X, Y):\n",
        "        x_value = torch.tensor(np.array(X).astype(np.float32))\n",
        "        answer = self.net(x_value)\n",
        "        results = [np.argmax(v.detach().numpy()) for v in self.transfer(answer)]\n",
        "        total_success = 0\n",
        "        for res, y in zip(results, Y):\n",
        "            if res == y:\n",
        "                total_success += 1\n",
        "        acc = total_success / len(Y)\n",
        "        plt.plot(self.level, acc, label='Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        return acc\n",
        "\n",
        "    def confusion_matrix(self, X, Y):\n",
        "        x_value = torch.tensor(np.array(X).astype(np.float32))\n",
        "        answer = self.net(x_value)\n",
        "        results = [np.argmax(v.detach().numpy()) for v in self.transfer(answer)]\n",
        "        matrix = np.zeros((self.classes_count, self.classes_count), dtype=np.int32)\n",
        "        for res, y in zip(results, Y):\n",
        "            matrix[res][y] += 1\n",
        "        return matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpDE6uqfokpY"
      },
      "source": [
        "## Загрузка датасетов и подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовительные функции"
      ],
      "metadata": {
        "id": "sQ_oypvEuy-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_values(df):\n",
        "    return np.array(df).astype(np.float32)/255\n",
        "\n",
        "def count_of_keys(data):\n",
        "    unique = np.unique(data)\n",
        "    count = np.zeros(len(unique), dtype=np.int32)\n",
        "    for v in data:\n",
        "        count[v] += 1\n",
        "    return count\n",
        "\n",
        "def print_data(labels, features, string :str):\n",
        "    print(\"{}\".format(string))\n",
        "    print(\"Number of elements in the dataset : {}\".format(len(features[0])))\n",
        "    print(\"The number of attributes in the element : {}\".format(len(labels)))\n",
        "    print(\"Minimum attribute value  : {}\".format(np.min(features)))\n",
        "    print(\"Maximum attribute value  : {}\".format(np.max(features)))\n",
        "    print()\n",
        "            \n",
        "def analize(labels, features, string :str):\n",
        "    print_data(labels, features, string)\n",
        "\n",
        "#рисовалка графиков аккюраси"
      ],
      "metadata": {
        "id": "ufRfiXpKuyKx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST"
      ],
      "metadata": {
        "id": "T4aHCe6r--qn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Rh3KARokpY",
        "outputId": "fd34b9b2-f040-4e31-8b8f-559fd0870905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "Training sample from MNIST\n",
            "Number of elements in the dataset : 784\n",
            "The number of attributes in the element : 60000\n",
            "Minimum attribute value  : 0.0\n",
            "Maximum attribute value  : 1.0\n",
            "\n",
            "torch.Size([10000, 28, 28])\n",
            "Testing sample from MNIST\n",
            "Number of elements in the dataset : 784\n",
            "The number of attributes in the element : 10000\n",
            "Minimum attribute value  : 0.0\n",
            "Maximum attribute value  : 1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ],
      "source": [
        "mnist_train = datasets.MNIST(root = './data/', train = True, transform = transforms.ToTensor(), download=True)\n",
        "mnist_train_labels = mnist_train.train_labels;\n",
        "mnist_train_data_raw = mnist_train.train_data;\n",
        "\n",
        "print(mnist_train_data_raw.shape)\n",
        "mnist_train_data = convert_values(mnist_train_data_raw)\n",
        "mnist_train_data_reshape = mnist_train_data.reshape(60000, -1)\n",
        "\n",
        "analize(mnist_train_labels, mnist_train_data_reshape, \"Training sample from MNIST\")\n",
        "\n",
        "mnist_test = datasets.MNIST(root = './data/', train = False,transform = transforms.ToTensor(), download=True)\n",
        "mnist_test_lebels = mnist_test.train_labels;\n",
        "mnist_test_data_raw = mnist_test.train_data;\n",
        "\n",
        "print(mnist_test_data_raw.shape)\n",
        "mnist_test_data = convert_values(mnist_test_data_raw)\n",
        "mnist_test_data_reshape = mnist_test_data.reshape(10000, -1)\n",
        "\n",
        "analize(mnist_test_lebels, mnist_test_data_reshape, \"Testing sample from MNIST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXcJd_tWokpZ"
      },
      "source": [
        "### FasionMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EB6tii5hokpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6691b9-ebae-4e03-c495-aa8e0f18ae7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "Training sample from Fashion MNIST\n",
            "Number of elements in the dataset : 784\n",
            "The number of attributes in the element : 60000\n",
            "Minimum attribute value  : 0.0\n",
            "Maximum attribute value  : 1.0\n",
            "\n",
            "torch.Size([10000, 28, 28])\n",
            "Testing sample from Fashion MNIST\n",
            "Number of elements in the dataset : 784\n",
            "The number of attributes in the element : 10000\n",
            "Minimum attribute value  : 0.0\n",
            "Maximum attribute value  : 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist_train = datasets.FashionMNIST(root = './fdata/', train = True, transform = transforms.ToTensor(), download=True)\n",
        "fashion_mnist_train_labels = fashion_mnist_train.train_labels;\n",
        "fashion_mnist_train_data_raw = fashion_mnist_train.train_data;\n",
        "\n",
        "print(fashion_mnist_train_data_raw.shape)\n",
        "fashion_mnist_train_data = convert_values(fashion_mnist_train_data_raw)\n",
        "fashion_mnist_train_data_reshape = fashion_mnist_train_data.reshape(60000, -1)\n",
        "\n",
        "analize(fashion_mnist_train_labels, fashion_mnist_train_data_reshape, \"Training sample from Fashion MNIST\")\n",
        "\n",
        "fashion_mnist_test = datasets.MNIST(root = './fdata/', train = False,transform = transforms.ToTensor(), download=True)\n",
        "fashion_mnist_test_labels = fashion_mnist_test.train_labels;\n",
        "fashion_mnist_test_y_raw = fashion_mnist_test.train_data;\n",
        "\n",
        "print(fashion_mnist_test_y_raw.shape)\n",
        "fashion_mnist_test_y = convert_values(fashion_mnist_test_y_raw)\n",
        "fashion_mnist_test_y_reshape = fashion_mnist_test_y.reshape(10000, -1)\n",
        "\n",
        "analize(fashion_mnist_test_labels, fashion_mnist_test_y_reshape, \"Testing sample from Fashion MNIST\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR-10"
      ],
      "metadata": {
        "id": "LbgSwyYw1i5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет CIFAR-10 состоит из цветных изображений размером 32х32. Для картинок есть 10 категорий."
      ],
      "metadata": {
        "id": "rv9L6bpYRzue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Загрузка датасета"
      ],
      "metadata": {
        "id": "Q1C4SaH5TS7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 50000\n",
        "test_size = 10000\n",
        "\n",
        "cfar_train = datasets.CIFAR10(root = './cdata/', train = True, transform = transforms.ToTensor(), download=True)\n",
        "cfar_train_list =  torch.utils.data.DataLoader(cfar_train, batch_size=train_size, shuffle=True, num_workers=2)\n",
        "\n",
        "cfar_test = datasets.CIFAR10(root = './cdata/', train = False,transform = transforms.ToTensor(), download=True)\n",
        "cfar_test_list =  torch.utils.data.DataLoader(cfar_test, batch_size=test_size, shuffle=True, num_workers=2)\n",
        "\n",
        "train_dataiter = iter(cfar_train_list)\n",
        "cfar_train_images, cfar_train_labels = train_dataiter.next()\n",
        "cfar_train_images_reshape = cfar_train_images.reshape(50000,-1)\n",
        "\n",
        "print(cfar_train_images_reshape.shape)\n",
        "print(cfar_train_labels.shape)\n",
        "\n",
        "dataiter = iter(cfar_test_list)\n",
        "cfar_test_images, cfar_test_labels = dataiter.next()\n",
        "cfar_test_images_reshape = cfar_test_images.reshape(10000,-1)\n",
        "\n",
        "print(cfar_test_images_reshape.shape)\n",
        "print(cfar_test_labels.shape)\n",
        "\n",
        "cfar_train_images_reshape_array= np.array(cfar_train_images_reshape)\n",
        "cfar_test_images_reshape_array= np.array(cfar_test_images_reshape)\n",
        "\n",
        "analize(cfar_train_labels, cfar_train_images_reshape_array, \"ОTraining sample from CIFAR-10\")\n",
        "\n",
        "analize(cfar_test_labels, cfar_test_images_reshape_array, \"Testing sample from CIFAR-10\")"
      ],
      "metadata": {
        "id": "VK52Br1O1s4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "87585cf6-fb2f-453a-9380-e85d02c2e9ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-79903dcae9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcfar_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./cdata/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcfar_train_list\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfar_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# expand redirect chain if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_redirect_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_hops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_redirect_hops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# check if file is located on Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_get_redirect_url\u001b[0;34m(url, max_hops)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_hops\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение и тестирование сети  "
      ],
      "metadata": {
        "id": "POTme1e51t7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`In` - количество входных нейронов\n",
        "\n",
        "`Out` - количество выходных нейронов\n",
        "\n",
        "`Hidden layers` - число скрытых слоев\n",
        "\n",
        "`The breadth of hidden layers` - широта скрытых слоев"
      ],
      "metadata": {
        "id": "DLvuIJ-ZBqkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_accuracy(accuracy):\n",
        "    print(\"Network accuracy : {}\".format(accuracy))"
      ],
      "metadata": {
        "id": "OIASG0pE1w-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST"
      ],
      "metadata": {
        "id": "3TiNM_OMMDd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передаточная функция: \n",
        "\n",
        "Количество слоев:\n"
      ],
      "metadata": {
        "id": "c1btwCOyMRqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_layer = Perceptron_solution_pytorch(layers=[])\n",
        "null_layer.training(mnist_train_data_reshape, mnist_train_labels)\n",
        "print_accuracy(null_layer.accuracy(mnist_test_lebels,mnist_test_data_reshape))"
      ],
      "metadata": {
        "id": "-5O7j3r1B-f3",
        "outputId": "d6f0ec9e-0690-4bcb-eafc-cf7f033b4e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a8321e0fa945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnull_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron_solution_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnull_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_data_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnull_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_test_lebels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_test_data_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передаточная функция: \n",
        "\n",
        "Количество слоев:"
      ],
      "metadata": {
        "id": "3meJfe6nMhg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EwmBrw_QMiK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передаточная функция: \n",
        "\n",
        "Количество слоев:"
      ],
      "metadata": {
        "id": "c0muS9TmMijS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gInRtHBeMjMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передаточная функция: \n",
        "\n",
        "Количество слоев:"
      ],
      "metadata": {
        "id": "5lxX6TrbMj5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yAPJ4sWVMlB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "name": "Копия блокнота \"Solution.ipynb\"",
      "provenance": [],
      "collapsed_sections": [
        "TXcJd_tWokpZ",
        "LbgSwyYw1i5v"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}